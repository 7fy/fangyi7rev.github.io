<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Data Visualizaion]]></title>
    <url>%2F2018%2F05%2F17%2FVisualization%2F</url>
    <content type="text"><![CDATA[Information visualizationData visualization is the graphical display of abstract information for two purposes: sense-making(data analysis) and communication. spatial data physically-based data with an inherent spatial mappling spatial mapping is typically defined by nature abstract data deos not have an inherent spatial mapping sense making (process) trageted analysis: questions are well-defined exploratory: questions are still evolving interactivity is important communicate (final results) summarize findings presenting insights the use of computer-supported,interacitve, visual representations of abstract data to amplify cognition. the power of unaided human mind is highly overrated. Without external aids, memory, thought and reasoning are all constraint. but human intelligence is highly flexible and adaptive, superb at inventing procedures and objects that overcome its own limits. the real powers come from devicing external aids that enhance cognitive abilities. computer-generated mappings between data and visuals - as the data changes, the visualization changes - no manual adjusting is required interacitve - people can interact with the visualization process of creating visualization the data set collection of information that is the target of the analysis types (table, networks and trees, clusters, sets, lists) tables networks (specifiy relationship between two or multiple items apsects of data item: individual discrete entity - data point attribute: property of an item that can be specified or measured link: relationship between items static (items are available all at one, do not change)dynamic (items change over time, new items are added over time) attribute types##categorical data (discrete) nominal （名义/标定数据）discrete values in a single category no intrinsic ordering ordinal （有序）discrete valuess in a single category have an implicit, well defined ordering interval （区间数据）series of numerical rangessequential with implict ordering hierarchical (分层数据）multiple categories that associated in a parent to child relationshipeach item is associated with one higer level category, several lower-level categories quantitative data (continuous) measurements of magnitude math operations make sense typical realtions: ranking, ratio, correlation ordered &amp; quantitative data sequentialrange from minimum to maximum diverging two sequences pointing in opposite directions and meet at one point cyclicvalues wrap back to starting point marksbasic graphical elements visible on paper/ on screenhave a dimension point sepcific position no dimensions can have a size and shape line one dimension variable length/size can have width areatwo dimensions variable size: width and heigth visual variables (channels) position changes in horizontal &amp; vertical position ( 2D) order quantitative assciative selective length size changes in length, area, repetition order quantitative(differences in lenght are easier to estimate than differences in area) selective(limited 5) associatve (limit) length(limit) shape no order no quantitative associative selective length color (hue) no order no quantitative associative(depends on the number of distractors and spatial distribution) selective length (limited 7) color (value) order no quantitative associative (limited 7) selective (limited 7) length (limit) orientation/angle texture saturation transparency curvature movement flicker principlesexpressivenessthe visual encoding should express all of, and only, the information in the dataset attributes show ordered data in a way that our perceptual system intrinsically senses as ordered. show categorical data in a way that does not perceptually suggest ordering. effectiveness principlethe importance of the data attribute should be encoded with the most effective visual variable, in order to be noticeable. discriminability （辨别） selective: distinguish between items, do thing pop-out associative: can we identify similar items Accuracy how accurate is our perception of a visual variable? separability (分离性） interact with other visual variables interferences between different visual variablesvisualization process consider the types of attributes in your dataset identify the most important attributes to be represented (driven by task and questions) choose the viusal variables according to their effectiveness ranking #classifying visualiztion techniques based on the number of attributes univariate data (just a single number or attibute) bivariate data(data along two dimensions/variables) multivariate data(data along multiple dimensions)based on the spatial arrangement scatterplottwo quantitative attributes + one nominal attribute express quantitative values as point marks using ( horizontal and vertical position)providing overviews - characterizing distributions (outliers, extremes) - finding correlations between two attributes - distinguishing data points from each other becomes difficult in larger data sets scatterplot matrix explore pairwise correlation across multiple quantitative attributes find pairwise correlations (trends, outliers) attributes&lt;12 ,data points up to hundreds bar charts product categories &amp; scales one nominal attribute and one quantitative attribute (line marks)value attribute expressed through length and aligned in vertical position, levels of nominal dimension separated through horizontal positionlookup, compare values, presenting trendslimited to hunderds of key levels alphabetical ordering (good for lookup) value-driven ordering (good for seeing trends) stacked bar charts multidimensional table (one nominal attribute can be divided into several sub-categoris. part of a whole relationship lookup values find trends limits height of full bars and lowest sub-bars easy to compare comparison of stacked sub-bars more difficult: no shared baseline order is important, interactivity can help scalability limited to hundreds of primary key levels several to a dozen secondary key levels normalized stacked bar charts show relative proportions part of whole relationships, comparison across key attribute levels dot chart difference to scatter plots one quantitative and one ordinal attribute line chart follow directly from dot charts with a line connecting the different data points good for showing trends line emphasizes ordering &amp; continuity (do not use for categorical data) multiple area chartsstacked area graphsstream graphs closely related to stacked bar charts emphasis on continuity of the horizontal layers primary key: ordinal secondary key: nomial one quantitative value one derived attribute (for layer ordering) part of the whole relationship, finding trends lookup and comparsion can be difficult parallel coordinates visualizing many quantitative/ordinal attributes use for parallel axes positive correlation: parallel lines negative correlation: crossing lines finding correlations identifying ranges of particular attributes; extremes identifying outliers ordering of axes is crucial scale: large number of data points moderate number of attributes (12) radial bar charts one categorical key attribute; one quantitative value angle and length(size) angle as a visual variable is less accurate than rectilinear position angle is perceived cyclic rather linear. good for showing data inherent cycles or periodic patterns. pie chart one quantitative attribute, one categorical attribute area mark in radial layout, angle, arc length, area size part of whole realtionship scale(&lt;12) limit judge area, not angle (inaccurate judgement) areas are irregular order the slices from largest to smallest donut charts one quantitative attribute; one categorical attribute area mark in radial layout; anglel arc length; area size part of whole realtionship &lt; 12 categories polar area chart one quantitative attribute, one categorical attribute area mark + length; radial layout; no variation of angle part whole relationship &lt; 12 categories space efficiency only worse design than a pie chart is several of them, for then the viewer is asked to compare quantities located in spatial disarray both within and between pies. task analysis overview of basic activities that a person may want to do using the visualization to achieve their goalsunderstanding the basic task help us to - think about data transformation steps required to fulfil these tasks - think about the design and functionality of our visualization actions/activities define user goalschoices of actions are independent of their level ( high to low; analyse to query) analyze how is the visualization being used for analysis ( consume or produce) discover (explore information) goal - find new knowledge that was previously unknown (open-ended) investigation motivated by hypotheses, existing theories, hunches, or open-ended questions discovery may generate new questions or hypotheses may arise from serendipitous discoverires through the visualization design considerations for visualzation show data from different perspectives enable open-ended explotation through interactivity present (communicate existing knowledge) goal - communicate information that is already understood to an audience telling a story with data guiding an audience through a series of data aspects context: entertainment, education, decision making design considerations for visualzation focus on a particular aspect of the data clarity of visual representation; simplicity of interaction enjoy (communicate existing knowledge + make it explorable) goal - support casual encounters with a visualization information need is not as pressing motivation is driven by pure curiosity another consume produce (use visualization to generate new material) annotate (注释） textual annotations to add additional information to an existing visualization could be considered as a new data attribute and further visualized record capture (interaction with) visualization elements logging particular activities directly by manipulating a visualization deriving new data (attributes) (data transformation) can be facilitated through the visualization search (elements of interest) what kind of(visual) search is involved (lookup, locate, browse, explore) lookup know what we are looking for know where to look text queries (文本查询） may be most effective visualization can highlight the specific item visual variables that support ‘popout’ effect well (selective) locate know what looking for do not know where some exploration/mnavigation is required ( interactions. visual) browse do not know what are looking for know where to looking for visualization can help identify items with particular characteristics and compare them to hepl us hone in on a subset of interest. explore do not know what looking for do not know where have to look visualization can provide an overview of the information space so we can identify basic trends/ patterns to help us narrow our goal/question furtherquery what kind of queries need to be performed? (identify, compare, summarize) identify ( single target) compare (multiple targets) summarize ( full set of targets) Targets: points of interest aspects of the data that are of interest to the user encoding relationsarc diagrams encoding relations in 1D good for showing pairwise relationshipscircle arrangements less orderd than positioning easy to descirbe relationships using straight lines high number of nodes get messy fast hierarchical edge bundling (can solve messy)chord diagrams relationshiops between groups of entities node-link diagrams characterizing the topology of the network direct and indirect connections between nodes finding all possible paths from one node to the other finding all adjacement nodes to a target node central nodes that connect two sub-networks identifying outliers- nodes that are not well connected spatial layout follows the network topology design considerations make good use of available space links determine proximity(adjacement) avoid overlaps of nodes avoid edge crossings use of position groupings indicate connections but indirect use of position danger of misinterpreting meaning position : typically non-deterministic does not use spatial memory difficult with dynamic data limit visual complexity computational complexity adding quantitative attributesCircle size frequency of nodes importance of nodes (measured quantitatively) line width frequency of nodes importance of edges (measured quantitatively) matrix diagram finding cluster (order of keys matters) identifying symmetries undirected networks (show half the matrix) directed networks (show full matrix) highly scalable nodeTrix combining node-ling and matrix diagram encoding relationship in hierarchcial data each node has exactly one parent (root node) each node can have no or many children (leaf nodes) hierarchical node-link diagram a special type of network clear identification of hierarchical levels parent node/child relationships explore topology; locate path; design consideration vertical &amp; horizontal space considerations readability of labels triangular horizontal layout use of splines avoids overlap between nodes and edges gives larger sub-trees more spcae radial node-link diagram can be more compact less implication of order reading of text labels more difficult indented layout clear identification of hierarchical levels facilitates scaning of node labels more compact version of a vertical layout dendogram all leaf nodes are aligned easy identification of leaf nodes space-filling layoutsicicle plot/ adjacency diagram tree depth -&gt; vertical position relationships &amp; sibling order -&gt; horizontal position number/weight of leaves -&gt; width/horizontal length part of whole relationship radial space filling number/weight of leaves -&gt; angle/area size part of whole relationship containmnet layoutstreemaps nested layout of hierarchical relationships (quantitative) attribute of node -&gt; size understanding of node attribute values content/structure of leaf nodes not good for topology high scalability (one million leaf nodes, one million links)design considerations for layout general layout of the visualization vertical &amp; horizontal space considerations readability of labels character of the data in particular the vertical layout can imply order avoid edge crossings avoid overlap of nodes data-ink ratio (用最少的墨表达最丰富的咨询） proportion of data-driven ink (pixels) used in an information visualization maximize the data-ink ratio (within reason) erase non-data ink (within reason) erase redundant data-ink (within reason) be careful with these guidelines as redundancy and even non-data ink can improve understanding and sensemaking chart junk critically assess information visualization for non-data driven graphics/text/images/supporting structures (or redundant information)visual embellishments, not essentially necessary to understand the data. Excessive gridlines and labeling unnecessary 3D effects &amp; texturing It is alright to decorate construction but never construct decoration. - Augustus Pugin useful junk no difference in accuracy of interpretation better long-term recall rules of thumb of visualization designno unjustified 3Dexpressiveness &amp; effectivess of visual variables in 2D cues that convey depth disparity of depth we do not perceive the world in 3D Most visual information is about a 2D image plane Depth information is only a tiny fraction Occlusion occlusion is a powerful depth cue (visible objects are interpreted as closer) motion parallax (动作视差）（objects that are closer appear to move by faster Perspective distortion （透视扭曲） distant objects appear smaller and change their position on the image plane great for creating realistic looking images bad in the infromation visualziaton context Familiarity with sizes of certain objects Shadows and lighting shading can helo to estimate height with respect to ground introduction of visual mess possible occlusion of marks that actually represent data shadows can interfere with other viusal variables ( color hue, value) Atmospheric perspective conflict with color encoding Stereoscopic disparity (立体不一致） Based on the differences in two images created from two camera viewpoints do not interfere with other visual variables but not the strongest depth cue, espically making assumptions about distant objects Length of a line that expands onthe image plane is scaled linearly Length of a line that extends intothe scene (on z‐axis) is scalednon‐linearly (Most visual information is about a 2D image plan)（Depth information is only a tiny fraction） for abstract data, using 2D spatial position is more effective no unjustified 2DResolution over ImmersionResolution (number of pixels 分辨率） over immersion (sense of presence in a scene 在场景中的存在感) Eyes beat memorymemory &amp; attention long-tern lasts for lifetime; no clearly defined limit short-term lasts for several seconds; highly limited (human attention) cognitive load experience of when the limits of working memory have been reached Failing to absorb and process (some) of the presented informationvisual search tasks -&gt; high cognitvie load Vigilance is highly limited (找不出来了） animation in visualzation can cause cogintive load Transition/change from one view to another When the transition/change is focusing on a particular attribute/visual element summary side-by-side views for detailed comparison across many frames (limit nunber of frames) animation can be good for highlighting “drastic but focused changes” jump cuts can be good for singular small changes for detailed analysis, provide playback controlsOverview first, zoom &amp; filter, details on demand (Interactivity) Gain an overview of the entire collection Zoom in on items of interest Filter out uninteresting items Select an item or group and get details when neededResponsiveness is required intercation comes at its cost attention intercation (navigation takes time) visual feedback should be provided within less than 1s consider the different aspects contributing to latency(潜在因素） Get it right in black and whiteForm follows function (形式应该跟随功能）“When I am working ona problem, I never think aboutbeauty but when I have finished, if the solution is not beautiful, I know it is wrong.make effective use of space do not waste screen space balanced layout do not overcrowd the space encoding geospatial dataposition (visual variable) follows geospatial positionchoropleth maps(分区地图） good for data has been aggregated(sum) by location quantitative attribute -&gt;colour limit raw data (not normalized) size is not connected to data values variables not separable (interaction between color and area) graduated symbol map (等级符号地图） placing symbols on a map frees up size, shape and color as variables limits: conveying precise positions can be difficult(Shapes should be used that spread evenly ) overlap comparision can be difficult (lack of alignment) cartogram (统计地图） Distort the shape of geographic regions so that area size directly encodesa data attributedorling cartogram (道灵统计地图） Each geographic region is represented as non-overlapping circle （Geospatial positions may shift according to circle size）flow mapstreat spatial position as abstract data (text)tag cloud typically font size - frequency of a certain word Spatial layout can vary According to order of occurrence According to frequency Alphabetically According to available space pros and cons +intuitive to read -word sizes difficult to compare small words difficult to readwordle words are tightly packed effective use of space to fill a given shapevertical tag cloudparallel tag clouds (x-axis = categories)word tree coloreffective use of color in visualization opponent process theory There are six primary colours, arranged perceptually as opponent pairs, alongthree axes Black ‐ white Red ‐ green Yellow ‐ blue Six elementary colours color spaces Colour vision is defined by three different receptor types Numerical description of colour detectable by the human visual system RGB = Red, Green, Blue used for digital displays additive color model CMY = Cyan(蓝绿）, Magenta（紫）, Yellow Paint, dyes Subtractive colour model HSL = Hue, Saturation, Lightnes“colour” as a visual variable/channelluminace/value/brightness good for ordered data types high resolution good for shape and edage detection Low accuracy of luminance in non‐contiguous(adjacement) regions (surround, background matters) rules Avoid using grey scale as a method for representing more than 5 numerical value Ensure luminance contrast with the background when showing symbols, text or detailed graphics. saturation good for ordered data types low accuracy of saturation in non-cintiguous regions the surround matters rules Avoid using representing more than 5 numerical values. Use even less saturation levels when working with small marks. Use more saturated colours when colour coding small regions, areas, symbols orthin lines. ( 小点用重色，大点用浅色） Use less saturated colours for coding large areas hue good for categorical data low accuracy of distinguishing hue in non-contiguous regions the surround matters hues are more difficult to distinguish between small mark the higher the spatial frequency, the less saturated the colour rules do not use more than 6-7 hues use conventional(normal) hues that people know how to name transparency Encoding information by decreasing the opacity of a mark Is not separable from the other colour variables Strong interactions with luminance and saturation aesthetic considerations Complementary colours (strong contrast) Analogous colors (harmonious) triadic color harmony colormaps (颜色表） Mapping between colour and data values Categorical (nominal) “qualitative”colourmaps (Segmented colourmaps) Ordered colour maps sequential diverging rules limit to 6-12 levels, especially if regions are not contiguous level attribute (hierarchies) focus on most important one cosider other visual variables categorical colourmaps Use easily distinguishable colour Consider the luminance contrast against background colour Colour may have to be distinguishable in black and white Consider colour choices with regard to colour deficiency (green red) The appropriate colour map depends on the mark type(small size or large size - saturation) Be consistent in your colour mappings across different views ordered colourmaps(sequential) For ordered or quantitative data Use the magnitude channels of luminance/value or saturation Range from minimum to maximum value ordered colourmaps(diverging) For ordered or quantitative data two hues at the endpoints neutral color(中立色） as midpoint continuous colourmaps good for showing quantitative attributes (luminance saturation) Number of hues depends on the level of structure of the data to be emphasized lecture 8 75页图可以看 intercationwhy Data can be large and/or complex! we can not show everything at once A single view can only show one aspect of the data- usually there is more to consider interaction by intentionselect (individual data items, links, attributes, levels) to highlight aspects to get more details to rearrange the data in any way explore/navigate panning : common in maps direct walk: automated movement of focusreconfigure(rearrangement) (show me a different arrangement) sorting encode (show me a different representation) alter the visual representation alter colour encoding, scale of sizes, orientationabstract/elaborate/zoom (show me more or less detail) details on demand geometric zooming semantic zooming (比如地图缩放地点名具体）filter （show me something conditionally） Traditional filter widgets Selections within the visualization as filter inputconnect （show me related items） single view (position,saturation/transparency.details) Single view (Connecting edges between item) Multiple views: brushing &amp; linkingfaceting data (split data) Splitting up the data representations into Multiple juxtaposed and/or coordinate views layers Showing the data from multiple perspectives same attributes in different representations different attributes in different views latency &amp; interaction design (responsiveness is required) interaction cost visual feedback should be provided within less than 1 s consider the different aspects contributing to latency (考虑导致延迟的不同方面）？？？]]></content>
  </entry>
  <entry>
    <title><![CDATA[Random-coefficients-models]]></title>
    <url>%2F2018%2F05%2F13%2FRandom-coefficients-models%20%2F</url>
    <content type="text"><![CDATA[Random coefficients models![][1] THE random effecrs assumend come from a normal distribution with mean 0 and some variance-covariance matrix.![][2] model fitting ML and REML are typical choices and REML is preferred for more realistic variance estima- tes.Model selection works in the same way as for random intercept models. REML-based fit criteria can only used to discriminate between models with different random effects/variance components (the fixed effects part of the models being compared must be the same) ML-based fit criteria can be used to compare all models with each other (provided the input/response data is the same) we should look at correlationship between intercept random effect and slope random effect $ \rho $AICc for small samples![][3] model assessmentInternally studentized residuals [1]:http://m.qpic.cn/psb?/V121Q7A010s4Qs/exBEYSyDR.k74r.NmVNXUWZtMETkMXi1FbkmajHao54!/b/dAgBAAAAAAAA&amp;bo=YgUWAQAAAAADB1I!&amp;rf=viewer_4 [2]:http://m.qpic.cn/psb?/V121Q7A010s4Qs/TIOqgG.kF.kXkWlg7mdxWPZYiqN5P*Qa1eKqeH738aU!/b/dDIBAAAAAAAA&amp;bo=PgWcAAAAAAADB4U!&amp;rf=viewer_4 [3]:http://m.qpic.cn/psb?/V121Q7A010s4Qs/ZJ9CmfXeWGVmbR6.uBWftzd7IETTgLO1aa8tRsnM2d0!/b/dAgBAAAAAAAA&amp;bo=rAVWAQAAAAADB9w!&amp;rf=viewer_4]]></content>
  </entry>
  <entry>
    <title><![CDATA[Random-intercept-mixed-effect-models]]></title>
    <url>%2F2018%2F05%2F12%2FRandom-intercept-mixed-effect-models%2F</url>
    <content type="text"><![CDATA[Random intercept mixed effect modelsaverage behaviour is the same for all individuals. while the baseline levels differ across individuals. have some mean level common baseline to all individuals ( $\beta_0 $) each individual is assigend it’s own contribution to the response(u_0i) $$ y_{it} = \beta_0 + u_{0i} +\beta_1 * x_{1it} + e_{it} $$ u0 is the subject-specific ‘adjustment’ to the baseline/intercept parameter. assumend to come from some distrinbution with mean 0 and some variance. assume error to be normal with some correlation within individuals. ![][1]the G matrix is q * q ( q is the number of random effects in the model) maximum likelihoodlog likelihood function for these model is based on a multivariate Normal distribution. maximises the log likelihood for the variance parameter first while treating the fixed effects as known constants. after, the fixed effects estimates are found by treating the variance parameters as fixed and finding the values for the $\beta$ which maximise the log-likelihood. produces variance estimate too samll. this bias is greatest for small samples. Restricted Maximu, likelihoodeliminating the fixed effects from the likelihood inorder only defined the variance parameter produce unbiased variance estimates. model selectionnested modelscan be compared using LRTs (likelihood ratio tests) using ML or REML results.reduced model must be a special case of the full model( cam be obtained by imposing some constraints on the full model) ( set one or more covariance parameters to 0) for ML-based, this comparison is valid for models with different fixed effects and/or covariance structures. for REML-based model, comparison is based on restricted likelihood values: 1.discriminate between models with different covariance structures. 2 only works for same fixed effects can not compare different X matrices. non-nested modelsAIC or AICc(For correlated data the effective sample size (N) is not the same as the apparent sample size） under ML, the AIC/AICc can be used to compare models with different fixed effects and random effects models. under REML, use AIC/AICc to compare models with the same fixed effecrs structure. use REML based AICc to compare different error structures(independence/ar(1)/continous AR(1)) since the fixed effects(model for the mean) are the same. parameter interpretationpopulation average and subject-specific will be the same under normal errors with no link function. random effects predictionsrandom effects shrunk towards the mean, the random effects predictions will be less widely spread than the coefficients obtained by fitting the effects as fixed. the extent that shrinkage depends on the size of the random effects variance, if the random effects variance is small. the shrinkage is large. (the spread of subject-specific intercepts are known to be very simialr to each other and close to 0) if the random effects variance is large then the shrinkage is small (the spread of subject-specific intercepts are known to be very diffuse) the extent that shrinkage depends on the sample size within individuals(ni) shrinkage is less for subjests with many repeated measurements(compared to subjects with fewer observation) we have more information with large sample. some inferencestandard error may be too small that we assumes the variance is know for fixed and random effects’ standard error.bias will be large when : the variance parameters are highly uncertain the ratio of variance parameters compared with error variance is small(variance parameter/error variance) large imbalance in the data (lots of missing value) Kenward-Roger adjustment can correct robust variance estimator (empirical sandwich estimator) use the variance of the residuals directly in the calculation of se (small sample size will not good)rather than specilfying correlation structure in the errors, this take into account the observed covariance in the resisudals confidence intervals$$ estiamte \pm t-multiplier \times standard error $$ Wald test for significance tests. model assessmentpredictive powerploting the observed response data versus the population average and subject-specific fitted valuesconditional residual: ![][2] and uncontidional residual RESIDUAL ANALYSISsubject-specific residuals (r_it) should show normally distributed and patternless when consider against any covariates. Scaled residuals can be used to assess variance and correlation structure of the model. (assumed should be noramlly distributed and uncorrealted) For normal errors models (such as this), only the variance estimates are typically affected (while the fixed effects estimates are unbiased) if the random effects are non-normal. influence diagnosticinfluential subjects using likelihood distance/ likelihood displacement.(If they largely affect the estimates of the fixed effects the Cook’s D (D) statistic will be relatively large. but not all cases) [1]:http://m.qpic.cn/psb?/V121Q7A010s4Qs/jEAr.0DFwNXOLZd9c2qzZ2pqF4VygeMKrTg31dIckgI!/b/dGYBAAAAAAAA&amp;bo=MARWAgAAAAADB0I!&amp;rf=viewer_4 [2]:http://m.qpic.cn/psb?/V121Q7A010s4Qs/E99UHOWhSBW*kRdXIdgyBGQTG5QxQjsqmC3Vo.Otab0!/b/dDABAAAAAAAA&amp;bo=2gWoAQAAAAADB1Q!&amp;rf=viewer_4]]></content>
  </entry>
  <entry>
    <title><![CDATA[GEE]]></title>
    <url>%2F2018%2F05%2F11%2FGEES%2F</url>
    <content type="text"><![CDATA[Models for correlated dataGENMOD in SAS return standard linear modelreturn p-value corresponding to chi-square$$ \chi ^2 =(estimate/ standard error)^2$$ Population Averaged Models$$ y = X\beta +e $$ （data是以正确的顺序order过的）（在本例子中，就是把每个subject插到time order上） X- design matrix Np (p is the columns, intercept+coffiencet)$ \beta $ p1 vector of regression coefficientse = error matrix N1e ~ (0, $ \delta^2 $I)I is a N N diagonal matrix different models for the noisedivided into $ n_i n_i $ block-diagonal structure for N N error matrix.empirical autocorrelation functions to investigate corrleation between residuals within blocks. an example of AR(1) block. the correlation between measurements 1 hour apart ( in this case) is the correlation coefficient $ \rho $ 2 hours is $ \rho^2 $ (以此类推）this data set contains 2 individuals and 4 observations for each.in some cases, if the data scenario was different and a group of randomly selected individuals was measured just once each, then we might suspect that all observations within that group (medical centre) have some common correlation. the structure only has 1 associated parameter to estimate ( $ \rho $) GEEEs Generalized Estimating EquationsGEEs model the mean response and the association between repeated measures within individuals separately. obtain coeffients obtain standard errorwe can estimate ‘model-based’ standard errors using assumed correlation structure, but is riksy unless we knot the correlation structure is reasonable. ( if assumed structure is wrong, standard errors will systematically too large or too small) robust standard errors based on the product of the sums of the residuals within panels. negative within-panel correlation results in a smaller product of sums than would be obtained under independence. positive within-panel correlation results in a larger product of sums than would be obtained under independence. Note: It is not appropriate to choose one model over the other just because the standard errors are smaller! Note: if we use ‘empirical’ standard errors then we don’t need to rely on the model to give us good standard errors (in many cases). comparing results under different correlation structuresthe model-based standard errors (SEs) were often very different to the empirical standard errors (ESEs) 总共分了三种correlation matrixs: independent correlation structure, AR(1), unsructured.（怎么比较好坏？ 看se 和ese 对比，如果差不多说明合适） model selectionGEEs use QL (quasi-likelihood)QL based or QIC scores. QICu choose between models with different sets of covariates QIC(R) choose between models with the same covariates but different correlation structures. $ \ beta_R $ is the set of coefficients obtained under some specified correlation structure R model assessing for GEEs if the size of the panels is small and the data are complete, use the unstructured specification if the observations within panels are collected across time use a corrleation structure that also incudes a time dependence. if the observations are clustered (but not collected over time) use the exchangeeable/ compound symmetry structure. if the numbers of panels is small then the independence model may be best. we could not only choose model by QIC(R), should also consider the SE issue. assess the relationship is appropriate using cumulative sums of residuals over a set of co-ordinates. (systematic over or under-predition across the covariate being considered tell us a problem with linearity) the cumulative residuals based on the model are compared with sets of cumulative residuals which are likely to be obtained if the linear model is appropriate. a discrepancy between the simulated sets and the cumulative residuals for our model, give evidence that the model is inappropriate. this discrepancy between model residuals and those expected under a linear model return a p-value. (small p-value signify a problem with the linear model. p大linear) assessing influential points and individualsdeletion diagnostics assess influence of individual data points and influential individuals Cook’s distance measute assess points outlyingis a scaled measure of the distance between the coefficient vectors when the k-th group of observations is deleted from the analysis.independence working model does not appear to have any wildly large cook’s distance value.to reduce concerns about influential individuals, one or more individuals assessing predictive power$ R^2 $ or concordance correlation (concordance correlation coefficient measures the agreement between two variablesThe concordance correlation coefficient is nearly identical to some of the measures called intra-class correlations. ) hypothesis testingWald test (H0: $ \beta_j = 0 $) To test if one of the slope coefficients (βj) is zero]]></content>
  </entry>
  <entry>
    <title><![CDATA[Two_dimensional_smoothers]]></title>
    <url>%2F2018%2F05%2F10%2Ftwo-dimensional-smoothers%2F</url>
    <content type="text"><![CDATA[Two dimensional smootherspermit the smooth relationship between one covariate and the response to vary with another covariate in a flexible way - inculde interaction terms in the model. ##TPS Thin plate splinesemploy a global smoothing parameter that applies across the two dimensional surface. a set of knots are chosen from a set of spatial co-ordinates。 the euclidean/straight line distances between each set of spatial co-ordinates and these knot locations is found; this creates one column of distances first find the distance between knot and the spaatial locationthen use the straight-line distance to create columns of the basis matrix.- one column per knot. this is the column of the basis matrix for j-th knot, i-th transect and t-th observation.and then used in the linear predictor alongside the linear terms for the spatial co-ordinates and coefficients attached to any other covariates. estimation can proceed unpenalised or with a penalty term. the basis function increase the value with distance from each knot location. model selection non-nested models can be chosen by comparing AIC/BIC. automatic model selection is unable yet. dredge function in R can help tell smooth but not linearity. (要自己检查linearity） in some cases, tje euclidean based smoother return sily answer. (straight line distances between knots unsuitable, in the internal exlusion area 直线距离中间经过的地方可能因为一些原因观测不到任何，比如说有河，或者没观察等， 这个时候不能用直线距离，可以绕一下） two dimensional smoothers should accommodate: global patterns(applys across large parts of the survey area) local patterns(only apply in parts of the survey area) internal exclusion zones CReSS (complex region spatial smoother)CReSS based smoothers rely on sensible allocated knots and location, coefficients are estimated without penalty.the specficiation process is similar.the basis function is different from TPS basis. values in the columns decline with distance from each knot. SALSA2Dcan help automatic select knots, locations and range parameter.choices we have:addingmovingdropping knots location ( have each basis function centred about an ovserved spatial location for maximum support from the data) using ‘space-filling algorithm to choose knots location. ensure maximum coverage for gice number of knots. （knots not to be average, some are global some are local) knots number ( the algorithm tends to ‘converge’ on a similar number of knots for the final model (regardless of the starting number of knots)) effective range of each knot (the effective range of the basis includes at least one point and possibly all points.) K-fold cv based on omitting correlated blocks, can be used to choose between models with different numbers of knots ,rather than knots loaction for a particular knot number. CReSS based models are linear in their parameters and thus can be fitted using a routine fitting engine and SALSA2D can be used to choose model flexibility. we require confidence inter- vals about these differences (for any model) to make any conclusions about whether these changes are statistically significant - or could be due to chance. limitationstime consuming, good a[[roximations to highly uneven two dimensional functions. check assumption -residual independentempirical run test (inadequate for overdispersed models ) (manually obtain the reference distribution rather than using the normal distribution) acf plot for correlationrobust standard errors which allows corrleation.]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F05%2F08%2FMT5757%20(1)%2F</url>
    <content type="text"><![CDATA[MT5757 标签（空格分隔）： MT5757 Tips:ignore disperison may lead to include unsignificant covariate overdispersed data — quasi-AIC $$QAIC=-((2log-likelihood)/dispersion-parameter- estimate)+2k$$ k=the number of model coefficients (intercept and slope coefficients+1 for the dispersion parameter estimare). Model assumptions: Collinearity Mean-variance relationship Independent data Covariate relationships are well described as linear on the link scale Collinearityis a measure of the similarity between covariates and occurs when one of more of the explanatory variables can be written as a linear combination of the other explanatory variables. collinearity amounts to tring to fit a plane supported by a line fits the data about as well as any other plane. VIF variance inflation factors $$VIF=\cfrac{1}{(1-R^2)}$$ The closer $R^2$ is to 1, the higher the collinearity and the VIF.GVIFs - any of the covariates have more than one parameter associated with them. (df&gt;1) Influence the slope parameter | the size of the standard errors Model complexity: signal: the underlying mean function of the processnoise: the variablity of the points about the underlying mean function Overly complex modelshas many parameters and so will model both the signal and the noise in the input data.low bias and high variance Overly simplistic modelshas too few parameters and models neither the signal nor the noise elements in the input data very well.high bias and low The bias-variance trade-off the trainning sample is used to fit the model. the validation samples is used to estimate the expected prediction error for that model (since it is data unseen by the model). a model is then chosen by finding the lowest expected prediction error across the range of candidate models (which will likely vary in the number and content of the covariates) k-fold cross-validation: - splitting the data into k samples - leaving out some subset of the data - prediction the response values in the omitted set using the bulk of the data EPE (expected prediction error)$$\Sigma\Sigma(y^*-y)^2 $$ Ridge Regressionpenalise coefficients closer to 0 than they would be under ordinary R^2 / ML based estimation the variance of the fitted functions across several data sets from the same process is reduced, but at the cost of bias.￼￼￼￼￼ the loss function our aim is to minimize the loss function. in ridge regression, the penality term is $\lambda\Sigma\beta^2$. thus all covariates will be shrink close to 0 but never = 0. Laaso Regressionleast absolute shirnkage and selection operatorestimate covariate shrink towards 0 and can be estimated as exactly 0. when n&gt;p, ridge regression tends to return better predictions ifthere are high correlations amongst the covariates. when p&gt;n it selects n variables at most. The lasso method does not group predictors and so if there a group of highly correlated predictors, the lasso tends to select one ofthese variables from this group in an arbitrary way. Elastic Neta combination of lasso and ridge regression. Temporal autocorrelationif the model does not capture the patterns in the response data, then some of this pattern will remain in model residuals and present as positive/negeative autocorrelation. Runs Test （游程检验）Runs test can diagnose non-indepeendence in model residuals. this test assigns a sign to each residual: positive residuals are labelled +1 amd negative residuals are labelled -1. too few(long) runs provide evidence of positive correlation, while too many(short) runs provide evidence of negative correlation. negative correlation indicates the residuals have a periodicity change. （-+—+-+—+-+—+-+—+ 随机得太过规律） correlation(left) ------------------------------independent(right) the runs test compares the observed number of runs(T) with what’s expeected under independence E(T) and adjusted for the variance V(T) to give a test statistiv W which has a standard Normal(0,1) distribution. Values within (-2,+2) independence p&gt;0.05 Positive corrleation (Wz &lt; -2). Negative correlation (Wz &gt;2). Positive correlation affects models selection can lead us to conclude that irrelevant variables are important. some noise may regard as a part of data, therefore some unimportant variables are added. GEEs(Generalized Estimating Equations) can help Nonlinearity on the link scalethe model assumes the relationships between the covariates and the response on the link scale are linear. partial residual plots exhibit curvature indicate the relationship is not well discribed. Tukey’s test a quadratic term term can be fitted for each predictor (eg. Depth^2) in the regression model as a test for nonlinearity. z/t test. small p-value indicare nonlinearities on link scale. GAMs(Generalized additive models permit smooth functions can help)]]></content>
  </entry>
  <entry>
    <title><![CDATA[Generalized_Additive_models]]></title>
    <url>%2F2018%2F05%2F07%2FGeneralized-Additive-models%2F</url>
    <content type="text"><![CDATA[Generalized Additive modelsan extension of GLMs that can implement a wide range of nonlinear relatioships between the response variable and the covariates. polynomials $$ \eta_{it} = \beta_0 + \sum_{i=1}^d \beta_j(x_{ijt})^d $$ eg. cubic polynomial d=3 $ \eta = \beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^3 + \beta_4DC $ ￼￼￼ d*N ploynomial basis matrix model fittingvia least-squares/maximum likelihood/ quasi-likelihood limitations the number of basis matrix get very large quickly can diffcult to be estimated exhibit oscillatory behaviour for high degree evenly smooth, do not allow more flexibility global function each row contribute in same way the fitted curve for high depth values affected by the low depth values. Truncated power basis (截断幂）more locally defined. each column only has non-zero values between a certain x-value ( the j-th ‘knot’) and the maximum x-value. the added term only works at the last power (eg. x^3 is the highest power. k1,k2,k3 only follow x^3) Limitations the number of basis matrix get very large quickly can diffcult to be estimated the columns of the basis matrix can also be prohibitively collinear they are still global functions beyond k knot locations. the high depth values can still be affected to some extent by the relationship for the very low depth values. vice versa. （高的地方不够高低的地方不够低） B-splinespiecewise polynomial functions which join smoothly at ‘knots’. model flexibility: the higher the degree the number of knots the location of knot (specifying knots close together results in amore flexible function) knots place with support from the data ( in areas with observations) where the biggest changes are likely to occur in the covariate relationship so there are at least 3 unique covariate values between them (to enable estimation of the curve between knots) (如果离太近，两个knots之间没有data或者data太少） compare with truncated power serires the value in the basis matrix are constrained to lie between 0 to 1. void estimation issues. prohibitively high collinearity is avoided because the columns of the basis only contribute non-zero values for parts of the x-range. model selection the flexibility of B-spline is fully dicated by the number of knots, or jointly determined by features and a penalty term. RS(regression spline) is fully determined by knots and their locations, PRS(penalised regression spline) and SS(smoothing spline) also employ a penalty during fitting process. Regression splines manual knot allocation : need to specify the degree of the basis, (the number of knots and the covariate values of the knots location.) more important benifitscovariate relationship which are unevenly smooth can easily be accommodated. several knots can be allocated to the covariate range where the flexibility requirements are greatest few knots can be allocatedto where the relationship is asily approximate by a quadratic/cubic fucntion(depending on the order of the basis) benifitsB spline can return highly nonlinear curves. but are linear in their parameters.can be fitted using ML or Ql. SALSAspatially adaptive local smoothing algorithm. – can automated model select the number and location of knots. works well for function require different amounts of flexibility in different parts of the covariate range. penalized regression splinein contrast of regression spline, PRS make some arbitrary choices for the number and location of knots and instead rely on a penalty term to control model flexibility.rely on a penalty term to control model flexibility. the degree of the basis function a large number of knots ( 10 knots is routine) which are invariably equally spaced using quantiles. a penalty term to control curve complexity the penalty term: - a smoothing parameter ($ \lambda $) user choosen by user - the coefficients from the model model fittingthe mininmised function j is the number of coefficients associated with the smooth term $ \lambda $ = 0 result in an interplotating function $ \lambda $ = $ \infty $ result in an straight line model selectionGCV (generalized cross-validation can use to choose smoothing parameter)estimate a different smoothing parameter for each covariate. GCV is a funtion of fitting the data based on an associated set of smoothing parameters and model complexity(edf) edf (effective degrees of freedom) quantifies the number of parametersx used to fit the model. (each penalized coefficient contributes less than 1 df. (big penalty for each covariate, edf should larger than 10) limitationseasy and quick to fit but golbal function. there fore if flexibility is required in the covariates relationship is uneven across the covariate range. smoothing splinesmoothing spline avoid the knots selection issue. allocating each unique x a knot and control flexibility at the fitting stage using a penalty term. penaltyinvolved squared secon derivative (根号二阶导数）the integrated squared second derivative describe the roughness of the curve. The bigger amplitude and steeper slope indicated rougher and larger first and second derivatives. penalty ensures the positive and negative second derivatives count equally. GCV can be used to fit the model local scoring algorithm starts all covariates at some initial position and then updataes each smooth term separately while fixing the other covariates in the model at their current position. limitationsmodels may not converge. time consuming. regression spline with automated model selcetion via SALSAspecify the model manually and choose both the number and the location of knots. SALSA also tries to improve model fit in a more radical way by switching each current knot location to the point returning the maximum Pearson residual.]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F04%2F18%2FXGBOOST%2F</url>
    <content type="text"><![CDATA[#苏梦园是大猪猪 Extreme Gradient Boosting FormatT苏梦园猪猪猪大猪猪苏梦园 The first-order Taylor expansion The second-order Taylor expansion Paramters]]></content>
  </entry>
</search>
