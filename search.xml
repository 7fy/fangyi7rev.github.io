<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Random-coefficients-models]]></title>
    <url>%2F2018%2F05%2F13%2FRandom-coefficients-models%20%2F</url>
    <content type="text"><![CDATA[Random coefficients models![][1] THE random effecrs assumend come from a normal distribution with mean 0 and some variance-covariance matrix.![][2] model fitting ML and REML are typical choices and REML is preferred for more realistic variance estima- tes.Model selection works in the same way as for random intercept models. REML-based fit criteria can only used to discriminate between models with different random effects/variance components (the fixed effects part of the models being compared must be the same) ML-based fit criteria can be used to compare all models with each other (provided the input/response data is the same) we should look at correlationship between intercept random effect and slope random effect $ \rho $AICc for small samples![][3] model assessmentInternally studentized residuals [1]:http://m.qpic.cn/psb?/V121Q7A010s4Qs/exBEYSyDR.k74r.NmVNXUWZtMETkMXi1FbkmajHao54!/b/dAgBAAAAAAAA&amp;bo=YgUWAQAAAAADB1I!&amp;rf=viewer_4 [2]:http://m.qpic.cn/psb?/V121Q7A010s4Qs/TIOqgG.kF.kXkWlg7mdxWPZYiqN5P*Qa1eKqeH738aU!/b/dDIBAAAAAAAA&amp;bo=PgWcAAAAAAADB4U!&amp;rf=viewer_4 [3]:http://m.qpic.cn/psb?/V121Q7A010s4Qs/ZJ9CmfXeWGVmbR6.uBWftzd7IETTgLO1aa8tRsnM2d0!/b/dAgBAAAAAAAA&amp;bo=rAVWAQAAAAADB9w!&amp;rf=viewer_4]]></content>
  </entry>
  <entry>
    <title><![CDATA[Random-intercept-mixed-effect-models]]></title>
    <url>%2F2018%2F05%2F12%2FRandom-intercept-mixed-effect-models%2F</url>
    <content type="text"><![CDATA[Random intercept mixed effect modelsaverage behaviour is the same for all individuals. while the baseline levels differ across individuals. have some mean level common baseline to all individuals ( $\beta_0 $) each individual is assigend it’s own contribution to the response(u_0i) $$ y_{it} = \beta_0 + u_{0i} +\beta_1 * x_{1it} + e_{it} $$ u0 is the subject-specific ‘adjustment’ to the baseline/intercept parameter. assumend to come from some distrinbution with mean 0 and some variance. assume error to be normal with some correlation within individuals. ![][1]the G matrix is q * q ( q is the number of random effects in the model) maximum likelihoodlog likelihood function for these model is based on a multivariate Normal distribution. maximises the log likelihood for the variance parameter first while treating the fixed effects as known constants. after, the fixed effects estimates are found by treating the variance parameters as fixed and finding the values for the $\beta$ which maximise the log-likelihood. produces variance estimate too samll. this bias is greatest for small samples. Restricted Maximu, likelihoodeliminating the fixed effects from the likelihood inorder only defined the variance parameter produce unbiased variance estimates. model selectionnested modelscan be compared using LRTs (likelihood ratio tests) using ML or REML results.reduced model must be a special case of the full model( cam be obtained by imposing some constraints on the full model) ( set one or more covariance parameters to 0) for ML-based, this comparison is valid for models with different fixed effects and/or covariance structures. for REML-based model, comparison is based on restricted likelihood values: 1.discriminate between models with different covariance structures. 2 only works for same fixed effects can not compare different X matrices. non-nested modelsAIC or AICc(For correlated data the effective sample size (N) is not the same as the apparent sample size） under ML, the AIC/AICc can be used to compare models with different fixed effects and random effects models. under REML, use AIC/AICc to compare models with the same fixed effecrs structure. use REML based AICc to compare different error structures(independence/ar(1)/continous AR(1)) since the fixed effects(model for the mean) are the same. parameter interpretationpopulation average and subject-specific will be the same under normal errors with no link function. random effects predictionsrandom effects shrunk towards the mean, the random effects predictions will be less widely spread than the coefficients obtained by fitting the effects as fixed. the extent that shrinkage depends on the size of the random effects variance, if the random effects variance is small. the shrinkage is large. (the spread of subject-specific intercepts are known to be very simialr to each other and close to 0) if the random effects variance is large then the shrinkage is small (the spread of subject-specific intercepts are known to be very diffuse) the extent that shrinkage depends on the sample size within individuals(ni) shrinkage is less for subjests with many repeated measurements(compared to subjects with fewer observation) we have more information with large sample. some inferencestandard error may be too small that we assumes the variance is know for fixed and random effects’ standard error.bias will be large when : the variance parameters are highly uncertain the ratio of variance parameters compared with error variance is small(variance parameter/error variance) large imbalance in the data (lots of missing value) Kenward-Roger adjustment can correct robust variance estimator (empirical sandwich estimator) use the variance of the residuals directly in the calculation of se (small sample size will not good)rather than specilfying correlation structure in the errors, this take into account the observed covariance in the resisudals confidence intervals$$ estiamte \pm t-multiplier \times standard error $$ Wald test for significance tests. model assessmentpredictive powerploting the observed response data versus the population average and subject-specific fitted valuesconditional residual: ![][2] and uncontidional residual RESIDUAL ANALYSISsubject-specific residuals (r_it) should show normally distributed and patternless when consider against any covariates. Scaled residuals can be used to assess variance and correlation structure of the model. (assumed should be noramlly distributed and uncorrealted) For normal errors models (such as this), only the variance estimates are typically affected (while the fixed effects estimates are unbiased) if the random effects are non-normal. influence diagnosticinfluential subjects using likelihood distance/ likelihood displacement.(If they largely affect the estimates of the fixed effects the Cook’s D (D) statistic will be relatively large. but not all cases) [1]:http://m.qpic.cn/psb?/V121Q7A010s4Qs/jEAr.0DFwNXOLZd9c2qzZ2pqF4VygeMKrTg31dIckgI!/b/dGYBAAAAAAAA&amp;bo=MARWAgAAAAADB0I!&amp;rf=viewer_4 [2]:http://m.qpic.cn/psb?/V121Q7A010s4Qs/E99UHOWhSBW*kRdXIdgyBGQTG5QxQjsqmC3Vo.Otab0!/b/dDABAAAAAAAA&amp;bo=2gWoAQAAAAADB1Q!&amp;rf=viewer_4]]></content>
  </entry>
  <entry>
    <title><![CDATA[Two_dimensional_smoothers]]></title>
    <url>%2F2018%2F05%2F10%2Ftwo-dimensional-smoothers%2F</url>
    <content type="text"><![CDATA[Two dimensional smootherspermit the smooth relationship between one covariate and the response to vary with another covariate in a flexible way - inculde interaction terms in the model. ##TPS Thin plate splinesemploy a global smoothing parameter that applies across the two dimensional surface. a set of knots are chosen from a set of spatial co-ordinates。 the euclidean/straight line distances between each set of spatial co-ordinates and these knot locations is found; this creates one column of distances first find the distance between knot and the spaatial locationthen use the straight-line distance to create columns of the basis matrix.- one column per knot. this is the column of the basis matrix for j-th knot, i-th transect and t-th observation.and then used in the linear predictor alongside the linear terms for the spatial co-ordinates and coefficients attached to any other covariates. estimation can proceed unpenalised or with a penalty term. the basis function increase the value with distance from each knot location. model selection non-nested models can be chosen by comparing AIC/BIC. automatic model selection is unable yet. dredge function in R can help tell smooth but not linearity. (要自己检查linearity） in some cases, tje euclidean based smoother return sily answer. (straight line distances between knots unsuitable, in the internal exlusion area 直线距离中间经过的地方可能因为一些原因观测不到任何，比如说有河，或者没观察等， 这个时候不能用直线距离，可以绕一下） two dimensional smoothers should accommodate: global patterns(applys across large parts of the survey area) local patterns(only apply in parts of the survey area) internal exclusion zones CReSS (complex region spatial smoother)CReSS based smoothers rely on sensible allocated knots and location, coefficients are estimated without penalty.the specficiation process is similar.the basis function is different from TPS basis. values in the columns decline with distance from each knot. SALSA2Dcan help automatic select knots, locations and range parameter.choices we have:addingmovingdropping knots location ( have each basis function centred about an ovserved spatial location for maximum support from the data) using ‘space-filling algorithm to choose knots location. ensure maximum coverage for gice number of knots. （knots not to be average, some are global some are local) knots number ( the algorithm tends to ‘converge’ on a similar number of knots for the final model (regardless of the starting number of knots)) effective range of each knot (the effective range of the basis includes at least one point and possibly all points.) K-fold cv based on omitting correlated blocks, can be used to choose between models with different numbers of knots ,rather than knots loaction for a particular knot number. CReSS based models are linear in their parameters and thus can be fitted using a routine fitting engine and SALSA2D can be used to choose model flexibility. we require confidence inter- vals about these differences (for any model) to make any conclusions about whether these changes are statistically significant - or could be due to chance. limitationstime consuming, good a[[roximations to highly uneven two dimensional functions. check assumption -residual independentempirical run test (inadequate for overdispersed models ) (manually obtain the reference distribution rather than using the normal distribution) acf plot for correlationrobust standard errors which allows corrleation.]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F05%2F08%2FGEES%2F</url>
    <content type="text"><![CDATA[title: GEEdate: 2018-05-11 01:30:48tags:[statistics]categories:statistics Models for correlated dataGENMOD in SAS return standard linear modelreturn p-value corresponding to chi-square$$ \chi ^2 =(estimate/ standard error)^2$$ Population Averaged Models$$ y = X\beta +e $$ （data是以正确的顺序order过的）（在本例子中，就是把每个subject插到time order上） X- design matrix Np (p is the columns, intercept+coffiencet)$ \beta $ p1 vector of regression coefficientse = error matrix N1e ~ (0, $ \delta^2 $I)I is a N N diagonal matrix different models for the noisedivided into $ n_i n_i $ block-diagonal structure for N N error matrix.empirical autocorrelation functions to investigate corrleation between residuals within blocks. an example of AR(1) block. the correlation between measurements 1 hour apart ( in this case) is the correlation coefficient $ \rho $ 2 hours is $ \rho^2 $ (以此类推）this data set contains 2 individuals and 4 observations for each.in some cases, if the data scenario was different and a group of randomly selected individuals was measured just once each, then we might suspect that all observations within that group (medical centre) have some common correlation. the structure only has 1 associated parameter to estimate ( $ \rho $) GEEEs Generalized Estimating EquationsGEEs model the mean response and the association between repeated measures within individuals separately. obtain coeffients obtain standard errorwe can estimate ‘model-based’ standard errors using assumed correlation structure, but is riksy unless we knot the correlation structure is reasonable. ( if assumed structure is wrong, standard errors will systematically too large or too small) robust standard errors based on the product of the sums of the residuals within panels. negative within-panel correlation results in a smaller product of sums than would be obtained under independence. positive within-panel correlation results in a larger product of sums than would be obtained under independence. Note: It is not appropriate to choose one model over the other just because the standard errors are smaller! Note: if we use ‘empirical’ standard errors then we don’t need to rely on the model to give us good standard errors (in many cases). comparing results under different correlation structuresthe model-based standard errors (SEs) were often very different to the empirical standard errors (ESEs) 总共分了三种correlation matrixs: independent correlation structure, AR(1), unsructured.（怎么比较好坏？ 看se 和ese 对比，如果差不多说明合适） model selectionGEEs use QL (quasi-likelihood)QL based or QIC scores. QICu choose between models with different sets of covariates QIC(R) choose between models with the same covariates but different correlation structures. $ \ beta_R $ is the set of coefficients obtained under some specified correlation structure R model assessing for GEEs if the size of the panels is small and the data are complete, use the unstructured specification if the observations within panels are collected across time use a corrleation structure that also incudes a time dependence. if the observations are clustered (but not collected over time) use the exchangeeable/ compound symmetry structure. if the numbers of panels is small then the independence model may be best. we could not only choose model by QIC(R), should also consider the SE issue. assess the relationship is appropriate using cumulative sums of residuals over a set of co-ordinates. (systematic over or under-predition across the covariate being considered tell us a problem with linearity) the cumulative residuals based on the model are compared with sets of cumulative residuals which are likely to be obtained if the linear model is appropriate. a discrepancy between the simulated sets and the cumulative residuals for our model, give evidence that the model is inappropriate. this discrepancy between model residuals and those expected under a linear model return a p-value. (small p-value signify a problem with the linear model. p大linear) assessing influential points and individualsdeletion diagnostics assess influence of individual data points and influential individuals Cook’s distance measute assess points outlyingis a scaled measure of the distance between the coefficient vectors when the k-th group of observations is deleted from the analysis.independence working model does not appear to have any wildly large cook’s distance value.to reduce concerns about influential individuals, one or more individuals assessing predictive power$ R^2 $ or concordance correlation (concordance correlation coefficient measures the agreement between two variablesThe concordance correlation coefficient is nearly identical to some of the measures called intra-class correlations. ) hypothesis testingWald test (H0: $ \beta_j = 0 $) To test if one of the slope coefficients (βj) is zero]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F05%2F08%2FMT5757%20(1)%2F</url>
    <content type="text"><![CDATA[title:MT5757date:2018-10-28 21:32:21type:”categories”layout:”categories”tags:[statistics, overdisperison]categories:statistics MT5757 Tips:ignore disperison may lead to include unsignificant covariate overdispersed data — quasi-AIC $$QAIC=-((2log-likelihood)/dispersion-parameter- estimate)+2k$$ k=the number of model coefficients (intercept and slope coefficients+1 for the dispersion parameter estimare). Model assumptions: Collinearity Mean-variance relationship Independent data Covariate relationships are well described as linear on the link scale Collinearityis a measure of the similarity between covariates and occurs when one of more of the explanatory variables can be written as a linear combination of the other explanatory variables. collinearity amounts to tring to fit a plane supported by a line fits the data about as well as any other plane. VIF variance inflation factors $$VIF=\cfrac{1}{(1-R^2)}$$ The closer $R^2$ is to 1, the higher the collinearity and the VIF.GVIFs - any of the covariates have more than one parameter associated with them. (df&gt;1) Influence the slope parameter | the size of the standard errors Model complexity: signal: the underlying mean function of the processnoise: the variablity of the points about the underlying mean function Overly complex modelshas many parameters and so will model both the signal and the noise in the input data.low bias and high variance Overly simplistic modelshas too few parameters and models neither the signal nor the noise elements in the input data very well.high bias and low The bias-variance trade-off the trainning sample is used to fit the model. the validation samples is used to estimate the expected prediction error for that model (since it is data unseen by the model). a model is then chosen by finding the lowest expected prediction error across the range of candidate models (which will likely vary in the number and content of the covariates) k-fold cross-validation: - splitting the data into k samples - leaving out some subset of the data - prediction the response values in the omitted set using the bulk of the data EPE (expected prediction error)$$\Sigma\Sigma(y^*-y)^2 $$ Ridge Regressionpenalise coefficients closer to 0 than they would be under ordinary R^2 / ML based estimation the variance of the fitted functions across several data sets from the same process is reduced, but at the cost of bias.￼￼￼￼￼ the loss function our aim is to minimize the loss function. in ridge regression, the penality term is $\lambda\Sigma\beta^2$. thus all covariates will be shrink close to 0 but never = 0. Laaso Regressionleast absolute shirnkage and selection operatorestimate covariate shrink towards 0 and can be estimated as exactly 0. when n&gt;p, ridge regression tends to return better predictions ifthere are high correlations amongst the covariates. when p&gt;n it selects n variables at most. The lasso method does not group predictors and so if there a group of highly correlated predictors, the lasso tends to select one ofthese variables from this group in an arbitrary way. Elastic Neta combination of lasso and ridge regression. Temporal autocorrelationif the model does not capture the patterns in the response data, then some of this pattern will remain in model residuals and present as positive/negeative autocorrelation. Runs Test （游程检验）Runs test can diagnose non-indepeendence in model residuals. this test assigns a sign to each residual: positive residuals are labelled +1 amd negative residuals are labelled -1. too few(long) runs provide evidence of positive correlation, while too many(short) runs provide evidence of negative correlation. negative correlation indicates the residuals have a periodicity change. （-+—+-+—+-+—+-+—+ 随机得太过规律） correlation(left) ------------------------------independent(right) the runs test compares the observed number of runs(T) with what’s expeected under independence E(T) and adjusted for the variance V(T) to give a test statistiv W which has a standard Normal(0,1) distribution. Values within (-2,+2) independence p&gt;0.05 Positive corrleation (Wz &lt; -2). Negative correlation (Wz &gt;2). Positive correlation affects models selection can lead us to conclude that irrelevant variables are important. some noise may regard as a part of data, therefore some unimportant variables are added. GEEs(Generalized Estimating Equations) can help Nonlinearity on the link scalethe model assumes the relationships between the covariates and the response on the link scale are linear. partial residual plots exhibit curvature indicate the relationship is not well discribed. Tukey’s test a quadratic term term can be fitted for each predictor (eg. Depth^2) in the regression model as a test for nonlinearity. z/t test. small p-value indicare nonlinearities on link scale. GAMs(Generalized additive models permit smooth functions can help)]]></content>
  </entry>
  <entry>
    <title><![CDATA[Generalized_Additive_models]]></title>
    <url>%2F2018%2F05%2F07%2FGeneralized-Additive-models%2F</url>
    <content type="text"><![CDATA[Generalized Additive modelsan extension of GLMs that can implement a wide range of nonlinear relatioships between the response variable and the covariates. polynomials $$ \eta_{it} = \beta_0 + \sum_{i=1}^d \beta_j(x_{ijt})^d $$ eg. cubic polynomial d=3 $ \eta = \beta_0 + \beta_1x + \beta_2x^2 + \beta_3x^3 + \beta_4DC $ ￼￼￼ d*N ploynomial basis matrix model fittingvia least-squares/maximum likelihood/ quasi-likelihood limitations the number of basis matrix get very large quickly can diffcult to be estimated exhibit oscillatory behaviour for high degree evenly smooth, do not allow more flexibility global function each row contribute in same way the fitted curve for high depth values affected by the low depth values. Truncated power basis (截断幂）more locally defined. each column only has non-zero values between a certain x-value ( the j-th ‘knot’) and the maximum x-value. the added term only works at the last power (eg. x^3 is the highest power. k1,k2,k3 only follow x^3) Limitations the number of basis matrix get very large quickly can diffcult to be estimated the columns of the basis matrix can also be prohibitively collinear they are still global functions beyond k knot locations. the high depth values can still be affected to some extent by the relationship for the very low depth values. vice versa. （高的地方不够高低的地方不够低） B-splinespiecewise polynomial functions which join smoothly at ‘knots’. model flexibility: the higher the degree the number of knots the location of knot (specifying knots close together results in amore flexible function) knots place with support from the data ( in areas with observations) where the biggest changes are likely to occur in the covariate relationship so there are at least 3 unique covariate values between them (to enable estimation of the curve between knots) (如果离太近，两个knots之间没有data或者data太少） compare with truncated power serires the value in the basis matrix are constrained to lie between 0 to 1. void estimation issues. prohibitively high collinearity is avoided because the columns of the basis only contribute non-zero values for parts of the x-range. model selection the flexibility of B-spline is fully dicated by the number of knots, or jointly determined by features and a penalty term. RS(regression spline) is fully determined by knots and their locations, PRS(penalised regression spline) and SS(smoothing spline) also employ a penalty during fitting process. Regression splines manual knot allocation : need to specify the degree of the basis, (the number of knots and the covariate values of the knots location.) more important benifitscovariate relationship which are unevenly smooth can easily be accommodated. several knots can be allocated to the covariate range where the flexibility requirements are greatest few knots can be allocatedto where the relationship is asily approximate by a quadratic/cubic fucntion(depending on the order of the basis) benifitsB spline can return highly nonlinear curves. but are linear in their parameters.can be fitted using ML or Ql. SALSAspatially adaptive local smoothing algorithm. – can automated model select the number and location of knots. works well for function require different amounts of flexibility in different parts of the covariate range. penalized regression splinein contrast of regression spline, PRS make some arbitrary choices for the number and location of knots and instead rely on a penalty term to control model flexibility.rely on a penalty term to control model flexibility. the degree of the basis function a large number of knots ( 10 knots is routine) which are invariably equally spaced using quantiles. a penalty term to control curve complexity the penalty term: - a smoothing parameter ($ \lambda $) user choosen by user - the coefficients from the model model fittingthe mininmised function j is the number of coefficients associated with the smooth term $ \lambda $ = 0 result in an interplotating function $ \lambda $ = $ \infty $ result in an straight line model selectionGCV (generalized cross-validation can use to choose smoothing parameter)estimate a different smoothing parameter for each covariate. GCV is a funtion of fitting the data based on an associated set of smoothing parameters and model complexity(edf) edf (effective degrees of freedom) quantifies the number of parametersx used to fit the model. (each penalized coefficient contributes less than 1 df. (big penalty for each covariate, edf should larger than 10) limitationseasy and quick to fit but golbal function. there fore if flexibility is required in the covariates relationship is uneven across the covariate range. smoothing splinesmoothing spline avoid the knots selection issue. allocating each unique x a knot and control flexibility at the fitting stage using a penalty term. penaltyinvolved squared secon derivative (根号二阶导数）the integrated squared second derivative describe the roughness of the curve. The bigger amplitude and steeper slope indicated rougher and larger first and second derivatives. penalty ensures the positive and negative second derivatives count equally. GCV can be used to fit the model local scoring algorithm starts all covariates at some initial position and then updataes each smooth term separately while fixing the other covariates in the model at their current position. limitationsmodels may not converge. time consuming. regression spline with automated model selcetion via SALSAspecify the model manually and choose both the number and the location of knots. SALSA also tries to improve model fit in a more radical way by switching each current knot location to the point returning the maximum Pearson residual.]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F04%2F18%2FXGBOOST%2F</url>
    <content type="text"><![CDATA[#苏梦园是大猪猪 Extreme Gradient Boosting FormatT苏梦园猪猪猪大猪猪苏梦园 The first-order Taylor expansion The second-order Taylor expansion Paramters]]></content>
  </entry>
</search>
