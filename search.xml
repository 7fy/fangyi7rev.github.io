<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title></title>
    <url>%2F2018%2F05%2F08%2FMT5757%20(1)%2F</url>
    <content type="text"><![CDATA[MT5757 标签（空格分隔）： MT5757 Tips:ignore disperison may lead to include unsignificant covariate overdispersed data — quasi-AIC $$QAIC=-((2log-likelihood)/dispersion-parameter- estimate)+2k$$ k=the number of model coefficients (intercept and slope coefficients+1 for the dispersion parameter estimare). Model assumptions: Collinearity Mean-variance relationship Independent data Covariate relationships are well described as linear on the link scale Collinearityis a measure of the similarity between covariates and occurs when one of more of the explanatory variables can be written as a linear combination of the other explanatory variables. collinearity amounts to tring to fit a plane supported by a line fits the data about as well as any other plane. VIF variance inflation factors $$VIF=\cfrac{1}{(1-R^2)}$$ The closer $R^2$ is to 1, the higher the collinearity and the VIF.GVIFs - any of the covariates have more than one parameter associated with them. (df&gt;1) Influence the slope parameter | the size of the standard errors Model complexity: signal: the underlying mean function of the processnoise: the variablity of the points about the underlying mean function Overly complex modelshas many parameters and so will model both the signal and the noise in the input data.low bias and high variance Overly simplistic modelshas too few parameters and models neither the signal nor the noise elements in the input data very well.high bias and low The bias-variance trade-off the trainning sample is used to fit the model. the validation samples is used to estimate the expected prediction error for that model (since it is data unseen by the model). a model is then chosen by finding the lowest expected prediction error across the range of candidate models (which will likely vary in the number and content of the covariates) k-fold cross-validation: - splitting the data into k samples - leaving out some subset of the data - prediction the response values in the omitted set using the bulk of the data EPE (expected prediction error)$$\Sigma\Sigma(y^*-y)^2 $$ Ridge Regressionpenalise coefficients closer to 0 than they would be under ordinary R^2 / ML based estimation the variance of the fitted functions across several data sets from the same process is reduced, but at the cost of bias.￼￼￼￼￼ the loss function our aim is to minimize the loss function. in ridge regression, the penality term is $\lambda\Sigma\beta^2$. thus all covariates will be shrink close to 0 but never = 0. Laaso Regressionleast absolute shirnkage and selection operatorestimate covariate shrink towards 0 and can be estimated as exactly 0. when n&gt;p, ridge regression tends to return better predictions ifthere are high correlations amongst the covariates. when p&gt;n it selects n variables at most. The lasso method does not group predictors and so if there a group of highly correlated predictors, the lasso tends to select one ofthese variables from this group in an arbitrary way. Elastic Neta combination of lasso and ridge regression. Temporal autocorrelationif the model does not capture the patterns in the response data, then some of this pattern will remain in model residuals and present as positive/negeative autocorrelation. Runs Test （游程检验）Runs test can diagnose non-indepeendence in model residuals. this test assigns a sign to each residual: positive residuals are labelled +1 amd negative residuals are labelled -1. too few(long) runs provide evidence of positive correlation, while too many(short) runs provide evidence of negative correlation. negative correlation indicates the residuals have a periodicity change. （-+—+-+—+-+—+-+—+ 随机得太过规律） correlation(left) ------------------------------independent(right) the runs test compares the observed number of runs(T) with what’s expeected under independence E(T) and adjusted for the variance V(T) to give a test statistiv W which has a standard Normal(0,1) distribution. Values within (-2,+2) independence p&gt;0.05 Positive corrleation (Wz &lt; -2). Negative correlation (Wz &gt;2). Positive correlation affects models selection can lead us to conclude that irrelevant variables are important. some noise may regard as a part of data, therefore some unimportant variables are added. GEEs(Generalized Estimating Equations) can help Nonlinearity on the link scalethe model assumes the relationships between the covariates and the response on the link scale are linear. partial residual plots exhibit curvature indicate the relationship is not well discribed. Tukey’s test a quadratic term term can be fitted for each predictor (eg. Depth^2) in the regression model as a test for nonlinearity. z/t test. small p-value indicare nonlinearities on link scale. GAMs(Generalized additive models permit smooth functions can help)]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F04%2F18%2FXGBOOST%2F</url>
    <content type="text"><![CDATA[#苏梦园是大猪猪 Extreme Gradient Boosting FormatT苏梦园猪猪猪大猪猪苏梦园 The first-order Taylor expansion The second-order Taylor expansion Paramters]]></content>
  </entry>
</search>
